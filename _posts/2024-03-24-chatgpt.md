---
layout: post
title: ChatGPT is not just "next-word prediction"
published: false
---

A common belief about ChatGPT is that it simply predicts whatever word is most statistically likely to appear next in a sentence. In other words, LLMs just regurgitate their training data and cannot "think." This idea first became popular when ChatGPT first went viral in late 2022. Here's why I think it's wrong.

## It can't be that simple
If ChatGPT were nothing more than "next-word prediction", then why wasn't it made decades ago? After all, the necessary hardware (fancy GPUs) and training data (the internet) have been around for years. It is true that GPUs have gotten a lot better in the past few years, but that doesn't really matter. (Suppose that someone in 2015 had come up with a simplified form of GPT-3 using the much-worse hardware available at the time. Even if it took 10 minutes to run instead of 10 seconds, it would be valuable.)

The reason why ChatGPT couldn't have been invented 10 years ago isn't just because of better hardware or training data. What really happened is that there was a breakthrough	in model architectures, which started when a team of Google researchers published *Attention is All You Need* in 2017.

<figure>
  <img src="/assets/images/transformer.webp" alt="the Transformer architecture" class="center" />
  <figcaption>Fig. 1: The Transformer Architecture</figcaption>
</figure>
<br />

Clearly, the transformer architecture took a lot of tinkering and ingenuity to come up with: it required creativity. Saying that LLMs are simply regurgitators of training data implicitly denies that this creativity ever existed, and it downplays the role that key researchers (e.g. the authors of *Attention is All You Need*) played in bringing about these advancements.

## Nobody really knows what's going on
The truth is that nobody (not even Sam Altman!) really understands how LLMs work. If this is true, then how was this stuff invented in the first place? 

The answer is that you don't need to know *why* something works in order to know *how* it works. For example, the Wright Brothers flew at Kitty Hawk way before Aerospace Engineering was a formal discipline. The theory behind why airplanes work didn't come until much later.
Similarly, nobody right now really understands why the transformer architecture works. To quote Stephen Wolfram's *What Is ChatGPT Doing … and Why Does It Work?*, "this is just one of those things that’s been 'found to work.'"
I think Wolfram is right: right now AI is in a "tinkering" stage (prompt engineering, trying random architectures to see what works, feedback loops using human trainers) and will evolve into a theoretical disclipline later on.

## Bonus: word-reguritators are nothing new
Now for some interesting computational linguistics trivia: word-regurgitating algorithms actually have existed for a while, though most were nowhere near the level of modern LLMs. They were called a "context-free grammar" and have existed since the 1970s. Here's a sample sentence from SCIGen, a program that generates seemingly-coherent computer science research papers which are, in fact, completely full of nonsense:
> "We consider an algorithm consisting of n semaphores.
Any unproven synthesis of introspective methodologies will
clearly require that the well-known reliable algorithm for the
investigation of randomized algorithms by Zheng is in Co-NP;
our application is no different. The question is, will Rooter
satisfy all of these assumptions? No."

(Fun fact: the [below paper](https://pdos.csail.mit.edu/archive/scigen/) was actually accepted as a "non-reviewed paper" to the World Multiconference on Systemics, Cybernetics and Informatics in 2005.) 

In short, we give ChatGPT some credit: at the very least, it's miles ahead of SCIGen.
