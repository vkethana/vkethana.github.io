---
layout: post
title: ChatGPT is not just "next-word prediction"
---

A common belief about ChatGPT is that it simply predicts whatever word is most statistically likely to appear next in a sentence. In other words, LLMs just regurgitate their training data and cannot "think." This mental model became popular when ChatGPT first went viral in late 2022. Here's why I think it's wrong.
Interestingly, the algorithm I described above actually does exist. It's called a "context-free grammar" and it has been around since the 1970s. Here's a sample sentence from SCIGen, a program that generates seemingly-coherent computer science research papers which are, in fact, completely full of nonsense:
> "We consider an algorithm consisting of n semaphores.
Any unproven synthesis of introspective methodologies will
clearly require that the well-known reliable algorithm for the
investigation of randomized algorithms by Zheng is in Co-NP;
our application is no different. The question is, will Rooter
satisfy all of these assumptions? No."

## It can't be that simple
If ChatGPT were nothing more than "next-word prediction", then why wasn't it made decades ago? After all, the necessary hardware (fancy GPUs) and training data (the internet) have been around for years. It is true that GPUs have gotten a lot better in the past few years, but that doesn't really matter. (Suppose that someone in 2015 had come up with a simplified form of GPT-3 using the much-worse hardware available at the time. Even if it took 10 minutes to run instead of 10 seconds, it would be valuable.)

The reason why ChatGPT couldn't have been invented 10 years ago isn't just because of better hardware or training data. What really happened is that there was a breakthrough	in model architectures, which started when a team of Google researchers published Attention is All you Need in 2017.

<Insert figure of transformer architecture>
Clearly, the transformer architecture took a lot of tinkering and ingenuity to come up with: it required creativity. Saying that LLMs are simply regurgitators of training data implicitly denies that this creativity ever existed, and it downplays the role that key researchers (e.g. the authors of Attention is All You Need) played in bringing about these advancements.

## Nobody really knows what's going on
The truth is that nobody (not even Sam Altman!) really understands how LLMs work. If this is true, then how was this stuff invented in the first place? The answer is that you don't need to know *why* something works in order to know *how* it works. For example, the Wright Brothers flew at Kitty Hawk way before Aerospace Engineering was a formal discipline. The theory behind why airplanes work didn't come until much later.
Similarly, nobody right now really understands why the transformer architecture works. To quote Stephen Wolfram's *What Is ChatGPT Doing … and Why Does It Work?*, "this is just one of those things that’s been 'found to work.'"
If even Stephen Wolfram admits he doesn't fully understand ChatGPT, then I think the rest of us can rest easy.
