<!DOCTYPE html>
<html lang="en" data-theme="dark-poole">

  <head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />

<title>
	vkethana
</title>

<link rel="stylesheet" href="/assets/css/styles.css" />
<link rel="shortcut icon" href="/dark-poole/assets/favicon.ico" />
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>ChatGPT is not just “next-word prediction” | vkethana</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="ChatGPT is not just “next-word prediction”" />
<meta name="author" content="Vijay Kethanaboyina" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A common belief about ChatGPT is that it simply predicts whatever word is most statistically likely to appear next in a sentence. In other words, LLMs just regurgitate their training data and cannot “think.” This mental model became popular when ChatGPT first went viral in late 2022. Here’s why I think it’s wrong. Interestingly, the algorithm I described above actually does exist. It’s called a “context-free grammar” and it has been around since the 1970s. Here’s a sample sentence from SCIGen, a program that generates seemingly-coherent computer science research papers which are, in fact, completely full of nonsense: “We consider an algorithm consisting of n semaphores. Any unproven synthesis of introspective methodologies will clearly require that the well-known reliable algorithm for the investigation of randomized algorithms by Zheng is in Co-NP; our application is no different. The question is, will Rooter satisfy all of these assumptions? No.”" />
<meta property="og:description" content="A common belief about ChatGPT is that it simply predicts whatever word is most statistically likely to appear next in a sentence. In other words, LLMs just regurgitate their training data and cannot “think.” This mental model became popular when ChatGPT first went viral in late 2022. Here’s why I think it’s wrong. Interestingly, the algorithm I described above actually does exist. It’s called a “context-free grammar” and it has been around since the 1970s. Here’s a sample sentence from SCIGen, a program that generates seemingly-coherent computer science research papers which are, in fact, completely full of nonsense: “We consider an algorithm consisting of n semaphores. Any unproven synthesis of introspective methodologies will clearly require that the well-known reliable algorithm for the investigation of randomized algorithms by Zheng is in Co-NP; our application is no different. The question is, will Rooter satisfy all of these assumptions? No.”" />
<link rel="canonical" href="http://localhost:4000/chatgpt.html" />
<meta property="og:url" content="http://localhost:4000/chatgpt.html" />
<meta property="og:site_name" content="vkethana" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-03-24T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ChatGPT is not just “next-word prediction”" />
<meta name="twitter:site" content="@v_kethana" />
<meta name="twitter:creator" content="@Vijay Kethanaboyina" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Vijay Kethanaboyina","url":"https://vkethana.com"},"dateModified":"2024-03-24T00:00:00-07:00","datePublished":"2024-03-24T00:00:00-07:00","description":"A common belief about ChatGPT is that it simply predicts whatever word is most statistically likely to appear next in a sentence. In other words, LLMs just regurgitate their training data and cannot “think.” This mental model became popular when ChatGPT first went viral in late 2022. Here’s why I think it’s wrong. Interestingly, the algorithm I described above actually does exist. It’s called a “context-free grammar” and it has been around since the 1970s. Here’s a sample sentence from SCIGen, a program that generates seemingly-coherent computer science research papers which are, in fact, completely full of nonsense: “We consider an algorithm consisting of n semaphores. Any unproven synthesis of introspective methodologies will clearly require that the well-known reliable algorithm for the investigation of randomized algorithms by Zheng is in Co-NP; our application is no different. The question is, will Rooter satisfy all of these assumptions? No.”","headline":"ChatGPT is not just “next-word prediction”","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/chatgpt.html"},"url":"http://localhost:4000/chatgpt.html"}</script>
<!-- End Jekyll SEO tag -->


</head>


  <body>
    <div class="container content">
      <header class="masthead">
	<h3 class="masthead-title">
		<a href="/" title="Home">vkethana.com</a>

		<nav class="nav">
			
			<small><a href="/writings/">Writings</a></small>
			<small><a href="/reading_log/">Reading Log</a></small>
			
		</nav>
	</h3>
</header>


      <main>
        <h1 class='post-title'>ChatGPT is not just "next-word prediction"</h1>
<time datetime="2024-03-24T00:00:00-07:00" class="post-date">24 Mar 2024</time>
<hr />
<p>A common belief about ChatGPT is that it simply predicts whatever word is most statistically likely to appear next in a sentence. In other words, LLMs just regurgitate their training data and cannot “think.” This mental model became popular when ChatGPT first went viral in late 2022. Here’s why I think it’s wrong.
Interestingly, the algorithm I described above actually does exist. It’s called a “context-free grammar” and it has been around since the 1970s. Here’s a sample sentence from SCIGen, a program that generates seemingly-coherent computer science research papers which are, in fact, completely full of nonsense:</p>
<blockquote>
  <p>“We consider an algorithm consisting of n semaphores.
Any unproven synthesis of introspective methodologies will
clearly require that the well-known reliable algorithm for the
investigation of randomized algorithms by Zheng is in Co-NP;
our application is no different. The question is, will Rooter
satisfy all of these assumptions? No.”</p>
</blockquote>

<h2 id="it-cant-be-that-simple">It can’t be that simple</h2>
<p>If ChatGPT were nothing more than “next-word prediction”, then why wasn’t it made decades ago? After all, the necessary hardware (fancy GPUs) and training data (the internet) have been around for years. It is true that GPUs have gotten a lot better in the past few years, but that doesn’t really matter. (Suppose that someone in 2015 had come up with a simplified form of GPT-3 using the much-worse hardware available at the time. Even if it took 10 minutes to run instead of 10 seconds, it would be valuable.)</p>

<p>The reason why ChatGPT couldn’t have been invented 10 years ago isn’t just because of better hardware or training data. What really happened is that there was a breakthrough	in model architectures, which started when a team of Google researchers published Attention is All you Need in 2017.</p>

<Insert figure="" of="" transformer="" architecture="">
Clearly, the transformer architecture took a lot of tinkering and ingenuity to come up with: it required creativity. Saying that LLMs are simply regurgitators of training data implicitly denies that this creativity ever existed, and it downplays the role that key researchers (e.g. the authors of Attention is All You Need) played in bringing about these advancements.

## Nobody really knows what's going on
The truth is that nobody (not even Sam Altman!) really understands how LLMs work. If this is true, then how was this stuff invented in the first place? The answer is that you don't need to know *why* something works in order to know *how* it works. For example, the Wright Brothers flew at Kitty Hawk way before Aerospace Engineering was a formal discipline. The theory behind why airplanes work didn't come until much later.
Similarly, nobody right now really understands why the transformer architecture works. To quote Stephen Wolfram's *What Is ChatGPT Doing … and Why Does It Work?*, "this is just one of those things that’s been 'found to work.'"
If even Stephen Wolfram admits he doesn't fully understand ChatGPT, then I think the rest of us can rest easy.
</Insert>


      </main>

      <footer class="footer">
        <small>
          &copy;
          <time datetime="2024"
            >2024</time
          > Vijay Kiran Kethanaboyina. All rights reserved.
        </small>
      </footer>
      </div>
  </body>
</html>

