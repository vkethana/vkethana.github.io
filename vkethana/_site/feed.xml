<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-24T21:02:34-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">vkethana</title><subtitle>Vijay Kethanaboyina's personal blog</subtitle><author><name>Vijay Kethanaboyina</name></author><entry><title type="html">ChatGPT is not just “next-word prediction”</title><link href="http://localhost:4000/chatgpt.html" rel="alternate" type="text/html" title="ChatGPT is not just “next-word prediction”" /><published>2024-03-24T00:00:00-07:00</published><updated>2024-03-24T00:00:00-07:00</updated><id>http://localhost:4000/chatgpt</id><content type="html" xml:base="http://localhost:4000/chatgpt.html"><![CDATA[<p>A common belief about ChatGPT is that it simply predicts whatever word is most statistically likely to appear next in a sentence. In other words, LLMs just regurgitate their training data and cannot “think.” This idea first became popular when ChatGPT first went viral in late 2022. Here’s why I think it’s wrong.</p>

<h2 id="it-cant-be-that-simple">It can’t be that simple</h2>
<p>If ChatGPT were nothing more than “next-word prediction”, then why wasn’t it made decades ago? After all, the necessary hardware (fancy GPUs) and training data (the internet) have been around for years. It is true that GPUs have gotten a lot better in the past few years, but that doesn’t really matter. (Suppose that someone in 2015 had come up with a simplified form of GPT-3 using the much-worse hardware available at the time. Even if it took 10 minutes to run instead of 10 seconds, it would be valuable.)</p>

<p>The reason why ChatGPT couldn’t have been invented 10 years ago isn’t just because of better hardware or training data. What really happened is that there was a breakthrough	in model architectures, which started when a team of Google researchers published <em>Attention is All You Need</em> in 2017.</p>

<figure>
  <img src="/assets/images/transformer.webp" alt="the Transformer architecture" class="center" />
  <figcaption>Fig. 1: The Transformer Architecture</figcaption>
</figure>
<p><br /></p>

<p>Clearly, the transformer architecture took a lot of tinkering and ingenuity to come up with: it required creativity. Saying that LLMs are simply regurgitators of training data implicitly denies that this creativity ever existed, and it downplays the role that key researchers (e.g. the authors of <em>Attention is All You Need</em>) played in bringing about these advancements.</p>

<h2 id="nobody-really-knows-whats-going-on">Nobody really knows what’s going on</h2>
<p>The truth is that nobody (not even Sam Altman!) really understands how LLMs work. If this is true, then how was this stuff invented in the first place?</p>

<p>The answer is that you don’t need to know <em>why</em> something works in order to know <em>how</em> it works. For example, the Wright Brothers flew at Kitty Hawk way before Aerospace Engineering was a formal discipline. The theory behind why airplanes work didn’t come until much later.
Similarly, nobody right now really understands why the transformer architecture works. To quote Stephen Wolfram’s <em>What Is ChatGPT Doing … and Why Does It Work?</em>, “this is just one of those things that’s been ‘found to work.’”
I think Wolfram is right: right now AI is in a “tinkering” stage (prompt engineering, trying random architectures to see what works, feedback loops using human trainers) and will evolve into a theoretical disclipline later on.</p>

<h2 id="bonus-word-reguritators-are-nothing-new">Bonus: word-reguritators are nothing new</h2>
<p>Now for some interesting computational linguistics trivia: word-regurgitating algorithms actually have existed for a while, though most were nowhere near the level of modern LLMs. They were called a “context-free grammar” and have existed since the 1970s. Here’s a sample sentence from SCIGen, a program that generates seemingly-coherent computer science research papers which are, in fact, completely full of nonsense:</p>
<blockquote>
  <p>“We consider an algorithm consisting of n semaphores.
Any unproven synthesis of introspective methodologies will
clearly require that the well-known reliable algorithm for the
investigation of randomized algorithms by Zheng is in Co-NP;
our application is no different. The question is, will Rooter
satisfy all of these assumptions? No.”</p>
</blockquote>

<p>(Fun fact: the <a href="https://pdos.csail.mit.edu/archive/scigen/">below paper</a> was actually accepted as a “non-reviewed paper” to the World Multiconference on Systemics, Cybernetics and Informatics in 2005.)</p>

<p>In short, we give ChatGPT some credit: at the very least, it’s miles ahead of SCIGen.</p>]]></content><author><name>Vijay Kethanaboyina</name></author><summary type="html"><![CDATA[A common belief about ChatGPT is that it simply predicts whatever word is most statistically likely to appear next in a sentence. In other words, LLMs just regurgitate their training data and cannot “think.” This idea first became popular when ChatGPT first went viral in late 2022. Here’s why I think it’s wrong.]]></summary></entry><entry><title type="html">How (and why) I am Starting a Blog</title><link href="http://localhost:4000/starting_blog.html" rel="alternate" type="text/html" title="How (and why) I am Starting a Blog" /><published>2024-03-24T00:00:00-07:00</published><updated>2024-03-24T00:00:00-07:00</updated><id>http://localhost:4000/starting_blog</id><content type="html" xml:base="http://localhost:4000/starting_blog.html"><![CDATA[<p>(Inspired by <a href="https://guzey.com/personal/why-have-a-blog/">this post</a> by Alexey Guzey).</p>

<h2 id="why-i-started-a-blog">Why I Started a Blog</h2>
<h3 id="no-downside-infinite-upside">No Downside, Infinite Upside</h3>
<p>A few decade ago people were at the mercy of publishing companies and mass media, who would arbitrarily decide what could and couldn’t be said. People were discriminated against. Good ideas went unheard. It sucked. 
But the internet has changed things: it’s now really easy to get your ideas out there. Aside from purchasing the domain, it’s basically free (using hosting service like <a href="https://pages.github.com/">Github Pages</a> or <a href="https://www.netlify.com/">Netlify</a>).</p>
<h3 id="even-repeated-content-is-fine">Even Repeated Content is Fine</h3>
<p>Even sites that mostly repeat what other people say can be valuable. My thinking is that as I write more and more, my own ideas will start to emerge without me even trying. Every good idea has already been said, but since nobody was listening the first time around, someone else will have to say it again.</p>
<h3 id="ripple-effects">Ripple Effects</h3>
<p>One person starting a blog can trigger other people to do the same. In my case, some of my inspirations include <a href="Progress Good">https://www.arjunkhemani.com/about</a> (philosophy of science, economics, education), <a href="https://gwern.net/index">Gwern</a> (machine learning, metascience, and a ton of other stuff), and <a href="https://matt.might.net/">Matt Might</a> (theoretical computer science, precision medicine, free software).
My hope is that starting this website will encourage other people to do the same thing, and we can reverse the development of “web feudalism”: hosting your entire online presence underneath a larger platform like Substack, Twitter, or Reddit. These platforms aren’t bad – they are probably essential for reaching large audiences – but they should coexist alongside an independent, decentralized web.</p>
<h2 id="implementation">Implementation</h2>
<p>Originally, my website used a handwritten Python script which took a directory of markdown files, stored in a folder titled <code class="language-plaintext highlighter-rouge">md/</code>, and converted them into a folder of HTML files called <code class="language-plaintext highlighter-rouge">output/</code>.
The script was straightfoward (ChatGPT wrote part of it for me). In fact, it boiled down to just a few lines of code:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">markdown</span>

<span class="k">def</span> <span class="nf">md_to_html</span><span class="p">(</span><span class="n">md_file</span><span class="p">):</span>
    <span class="c1"># Read markdown content from file 
</span>		<span class="c1"># (Script assumes that blog content is written in Markdown)
</span>    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">md_file</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">md_content</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>

    <span class="c1"># Convert markdown to HTML
</span>    <span class="n">html_content</span> <span class="o">=</span> <span class="n">markdown</span><span class="p">.</span><span class="n">markdown</span><span class="p">(</span><span class="n">md_content</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">html_content</span>
</code></pre></div></div>
<p>Then I inserted the HTML output into a pre-made template file which had some extra amenities – a header, a footer, CSS styling…:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">insert_content_into_template</span><span class="p">(</span><span class="n">template_file</span><span class="p">,</span> <span class="n">content</span><span class="p">):</span>
    <span class="c1"># Read template content
</span>    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">template_file</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">template_content</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>

    <span class="c1"># Insert content into template
</span>    <span class="n">main_content_index</span> <span class="o">=</span> <span class="n">template_content</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">"&lt;!-- Main content --&gt;"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">main_content_index</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">output_content</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">template_content</span><span class="p">[:</span><span class="n">main_content_index</span><span class="p">]</span> <span class="o">+</span>
            <span class="s">"&lt;!-- Main content --&gt;"</span> <span class="o">+</span>
            <span class="n">content</span> <span class="o">+</span>
            <span class="n">template_content</span><span class="p">[</span><span class="n">main_content_index</span><span class="p">:]</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output_content</span> <span class="o">=</span> <span class="n">template_content</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span> <span class="o">+</span> <span class="n">content</span>

    <span class="k">return</span> <span class="n">output_content</span>
</code></pre></div></div>
<p>This approach worked fine, but it got clunky when I wanted to add multiple templates or insert figures/images.</p>
<h3 id="introducing-jekyll">Introducing Jekyll</h3>
<p>Jekyll fixes all these problems – easy templating, a large plugin ecosystem, the ability to insert HTML into Markdown files (a godsend if you’re trying to include graphs / images), you name it. The only price you pay is that you have to work with Ruby version management which is, well, messy. For example, I spent a whole afternoon trying to fix my buggy Homebrew installation of Ruby, which was preventing me from installing 3rd-party plugins. As far as I understand, Jekyll works best when you build up your site from scratch instead of using pre-made templates which deprecate quickly and often break mysteriously.</p>]]></content><author><name>Vijay Kethanaboyina</name></author><summary type="html"><![CDATA[(Inspired by this post by Alexey Guzey).]]></summary></entry><entry><title type="html">Language Learning</title><link href="http://localhost:4000/language_learning.html" rel="alternate" type="text/html" title="Language Learning" /><published>2024-03-04T00:00:00-08:00</published><updated>2024-03-04T00:00:00-08:00</updated><id>http://localhost:4000/language_learning</id><content type="html" xml:base="http://localhost:4000/language_learning.html"><![CDATA[<ul>
  <li>Duolingo more is useful for studying writing systems than languages.</li>
  <li>Memorize full phrases instead of individual words. Words on their own are not very useful because they are dismembered - you have no idea how to use them in context. By memorizing full sentences, you gradually build up intuition for when to use which word.</li>
  <li>If the language you’re trying to learn has its own writing system, learn it - even if you aren’t interested in reading or writing.</li>
  <li>Knowing IPA is really worth it, and it only takes one (or two) afternoons!</li>
  <li>If you want to practice forming sentences and you don’t have a native speaker to consult, you can try using Google Translate as follows. Configure the app to translate from the target language to English. Then, use the voice recognition tool to say sentences in the target language. If Google Translate can understand what you’re saying, there is a decent chance native speakers can, too. (Of course, this doesn’t guarantee that your sentences will sound natural.)</li>
  <li>Comprehensible input really does wonders. Listen to enough content in in your target language – at the right level of difficulty – and your brain just… figures it out.</li>
  <li>Michel Thomas’ audio courses are a great example of good language teaching. He doesn’t give you ten words and then ask you to repeat them, chastizing you if you don’t pronounce them with the perfect accent. Instead, he gives you a set of words and <em>you</em> have to figure out how to put them together. Skeptical? Try listening to <a href="https://youtu.be/rU56v5z3VZs?feature=shared">half an hour of his French course</a> and see how much you learn.</li>
  <li>For languages that don’t have a lot of resources, your best bet is documentation written by linguists, amateur or professional. Try to find texts that look like <a href="https://docs.google.com/document/d/1OHqWFBpQA4yOoNP1sIEi2QsfUQj1cmOZcD673BLdD1k/edit?pli=1#heading=h.cnjuj8jmii4m">this</a>.</li>
  <li>For those learning classical languages, you might find <a href="https://web.archive.org/web/20221208220810/https://www.wcdrutgers.net/Latin.htm"><em>Latin by the Dowling Method</em></a> interesting.</li>
  <li><a href="https://japaneselevelup.com/blog/">Japanese Level Up</a> is another good blog which talks about the philosophy behind successfully learning a language.</li>
  <li>Anki is really awesome.</li>
  <li>If you’re a college student and you’re deciding what language to take, consider picking the one that would be hardest to self-study.</li>
  <li>When you are measuring how hard it is to learn a language, it’s important to take into account the difficulty of finding learning materials, not just the difficulty of the language itself.</li>
  <li>We need more people studying classical languages.</li>
  <li>We need more people studying under-resourced languages.</li>
</ul>]]></content><author><name>Vijay Kethanaboyina</name></author><summary type="html"><![CDATA[Duolingo more is useful for studying writing systems than languages. Memorize full phrases instead of individual words. Words on their own are not very useful because they are dismembered - you have no idea how to use them in context. By memorizing full sentences, you gradually build up intuition for when to use which word. If the language you’re trying to learn has its own writing system, learn it - even if you aren’t interested in reading or writing. Knowing IPA is really worth it, and it only takes one (or two) afternoons! If you want to practice forming sentences and you don’t have a native speaker to consult, you can try using Google Translate as follows. Configure the app to translate from the target language to English. Then, use the voice recognition tool to say sentences in the target language. If Google Translate can understand what you’re saying, there is a decent chance native speakers can, too. (Of course, this doesn’t guarantee that your sentences will sound natural.) Comprehensible input really does wonders. Listen to enough content in in your target language – at the right level of difficulty – and your brain just… figures it out. Michel Thomas’ audio courses are a great example of good language teaching. He doesn’t give you ten words and then ask you to repeat them, chastizing you if you don’t pronounce them with the perfect accent. Instead, he gives you a set of words and you have to figure out how to put them together. Skeptical? Try listening to half an hour of his French course and see how much you learn. For languages that don’t have a lot of resources, your best bet is documentation written by linguists, amateur or professional. Try to find texts that look like this. For those learning classical languages, you might find Latin by the Dowling Method interesting. Japanese Level Up is another good blog which talks about the philosophy behind successfully learning a language. Anki is really awesome. If you’re a college student and you’re deciding what language to take, consider picking the one that would be hardest to self-study. When you are measuring how hard it is to learn a language, it’s important to take into account the difficulty of finding learning materials, not just the difficulty of the language itself. We need more people studying classical languages. We need more people studying under-resourced languages.]]></summary></entry><entry><title type="html">Free Ideas</title><link href="http://localhost:4000/free_ideas.html" rel="alternate" type="text/html" title="Free Ideas" /><published>2024-03-03T00:00:00-08:00</published><updated>2024-03-03T00:00:00-08:00</updated><id>http://localhost:4000/free_ideas</id><content type="html" xml:base="http://localhost:4000/free_ideas.html"><![CDATA[<p>1) Given a silent MRI video of somebody talking, is it possible to train an ML model to detect what language they are speaking?</p>

<p>2) Suppose there exist two languages, language X and language Y. X and Y are sufficiently different from each other to be considered separate languages, but they still have a lot of shared vocabulary (e.g. English/French, Spanish/Italian). What is the most efficient way to generate sentences in language X that have high mutual intelligiblity for speakers of language Y?</p>

<p>For example, the French sentence below is mostly intelligible to a speaker of English:</p>
<blockquote>
  <p>“Le président Emmanuel Macron assure le peuple canadien que le gouvernement français va continuer à défendre le Canada contre la menace américain.”</p>
</blockquote>

<p>Even if you didn’t catch any word, you can get the gist of it – the French	president Emmanuel Macron is assuring the “peuple canadien” (Canadian people) about something involving the “gouvernment français” (French government). Imagine reading thousands of sentences like this – it would be a great way to “backdoor” into a new language using cognates you already know. Solving this problem will probably involve NLP, statistics, and some kind of cognate detection tool. I’ve made a simple demo of this concept <a href="https://app.vkethana.com/">here</a>.</p>

<p>3) Is it possible to design a writing system that combines English consonant letters with Abugida-style vowel diacritics?	
For example, the letter “B” would be written “B” and the letter “BA” would be written “Bा. “BI”, “BO”, and “BU” would be  “िB” “Bो”, and “Bु” respectively. 
Here’s an example:</p>

<p><img src="/assets/images/abugida.jpeg" alt="A writing system combining English consonants with Hindi vowels" width="450" /></p>

<p>Here’s another example, with diacritics exclusively on top of the words:</p>

<p><img src="/assets/images/abugida2.jpeg" alt="A second version, which has all the diacritics on top" width="450" /></p>

<p>4) Is it possible to generate a constructed language using AI? If the language was more “concise” than English (e.g. it takes 150 characters to express a thought that would take 200 characters in English), would there be any practical value to it over English? (Douglas Hofstader alludes to this idea in <em>Godel, Escher, Bach</em> when he talks about translation between languages by means of an intermediate langauge as opposed to dictionary lookup.)</p>

<p>5) In the <em>Beginning of Infinity</em>, physicist David Deutsch proposes the following experiment: find some robot that is already used in the real world and happens to be able to walk. Replace the robot’s existing code with completely random code (“random numbers”, in his words) and implement a system that allows small bits of the code to randomly “mutate”, similar to genetic mutation. The idea behind using random numbers is to totally preclude the possibility that human knowledge is somehow being transfered to the robot. Given enough mutations and time, will the robot ever learn to walk? Has anybody every simulated this experiment?</p>]]></content><author><name>Vijay Kethanaboyina</name></author><summary type="html"><![CDATA[1) Given a silent MRI video of somebody talking, is it possible to train an ML model to detect what language they are speaking?]]></summary></entry><entry><title type="html">Five Different Takes on The Beginning of Infinity</title><link href="http://localhost:4000/beginning_of_infinity.html" rel="alternate" type="text/html" title="Five Different Takes on The Beginning of Infinity" /><published>2024-03-01T00:00:00-08:00</published><updated>2024-03-01T00:00:00-08:00</updated><id>http://localhost:4000/beginning_of_infinity</id><content type="html" xml:base="http://localhost:4000/beginning_of_infinity.html"><![CDATA[<p>Author Nassim Taleb once said that if a book is good, you should ask ten different people to summarize it and get ten, very different summaries. Inspired by this thought, I am writing out a list of summaries of David Deutsch’s <em>The Beginning of Infinity</em> from a variety of angles. The goal is to show just how multidimensional of a text it is, and to interest viewers from a variety of backgrounds to try reading the book.</p>

<h3 id="1-philosophy">1. Philosophy</h3>

<p>“The Beginning of Infinity is about the power of good explanations and how they form the backbone of modern science. Good explanations are hard to vary and make risky predictions.”</p>

<h3 id="2-physics--mathematics--computer-science">2. Physics / Mathematics / Computer Science</h3>

<p>The goal of the Beginning of Infinity is to show that if you want a deep understanding of proofs, computation, or infinity, it cannot come from just mathematical intuition. You have to build it up from the laws of physics.</p>

<h3 id="3-artificial-intelligence">3. Artificial Intelligence</h3>

<p>The Beginning of Infinity is really about the nature of intelligence &amp; how it is inextricably tied to creativity, explanations, &amp; knowledge creation. AGI hasn’t been achieved yet because there’s some deep fact about about creativity that we humans haven’t yet grasped.</p>

<h3 id="4-human-history">4. Human History</h3>

<p>The Beginning of Infinity is about the role that creativity and optimism play in society. Memes (ideas) form the backbone of a culture and determine its trajectory. Good explanations have the power to change society for the better &amp; prevent zero-sum thinking.</p>

<h3 id="5-biology">5. Biology</h3>

<p>(Unfortunately, I know very little about biology! If someone with a background in this area would like to contribute a summary, please let me know on Twitter <a href="https://twitter.com/v_kethana/status/1763315491828031498">@v_kethana</a>.)</p>]]></content><author><name>Vijay Kethanaboyina</name></author><summary type="html"><![CDATA[Author Nassim Taleb once said that if a book is good, you should ask ten different people to summarize it and get ten, very different summaries. Inspired by this thought, I am writing out a list of summaries of David Deutsch’s The Beginning of Infinity from a variety of angles. The goal is to show just how multidimensional of a text it is, and to interest viewers from a variety of backgrounds to try reading the book.]]></summary></entry></feed>